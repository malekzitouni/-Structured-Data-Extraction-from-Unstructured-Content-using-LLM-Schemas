# Local Mistral Model Runner

This Python script sets up and runs a local Mistral-7B-Instruct model using Hugging Face Transformers with memory-optimized settings for systems with limited VRAM.

## Overview

The script loads a pre-downloaded Mistral 7B instruction-tuned model from local cache and creates a text generation pipeline with 4-bit quantization to reduce memory usage from ~14GB to approximately 6GB VRAM.

## Key Features

### Memory Optimization
- **4-bit quantization**: Reduces model size by ~75% while maintaining reasonable performance
- **Float16 precision**: Uses half-precision floating point for additional memory savings
- **Auto device mapping**: Automatically utilizes GPU if available, falls back to CPU

### Model Configuration
- **Model**: Mistral-7B-Instruct-v0.2 (instruction-tuned variant)
- **Local path**: Loads from Hugging Face cache directory
- **Pipeline**: Uses text-generation pipeline for simplified inference

### Generation Parameters
- **Max tokens**: Limited to 100 new tokens for faster generation
- **Sampling**: Enabled with temperature=0.7 and top_p=0.9 for balanced creativity
- **Prompt**: Tests with an AI evolution question

## Dependencies
- `transformers`: Hugging Face model loading and pipeline creation
- `torch`: PyTorch backend for model inference
- Hardware: GPU with 6GB+ VRAM recommended (works on CPU but slower)

## Use Cases
Perfect for running large language models locally on consumer hardware, prototyping AI applications, or situations requiring offline inference capabilities.

